# -*- coding: utf-8 -*-
"""pump and dump

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jxr1VUoMjhlq1N_oFfSUxczc87NWkYb7
"""







import json
import datetime
import matplotlib.pyplot as plt


# --- Load the Combined Data ---
with open("../../combined_token_data.json", "r") as infile:
    combined_data = json.load(infile)

# Select the token to plot, e.g. "Tap"
token = "Verasity"
token_data = combined_data.get(token)
if not token_data:
    raise ValueError(f"Token {token} not found in combined data.")

# --- Extract and process CoinMarketCap historical price data ---
cmc_quotes = token_data["coinmarketcap"]["historical_data"]["quotes"]

cmc_times = []
prices = []
for quote in cmc_quotes:
    ts_str = quote["timestamp"]
    dt = datetime.datetime.fromisoformat(ts_str.replace("Z", "+00:00"))
    cmc_times.append(dt)
    prices.append(quote["quote"]["USD"]["price"])

# Define timeframe as the range of CoinMarketCap data
start_time = min(cmc_times)
end_time = max(cmc_times)

# --- Extract and process LunarCrush interaction data ---
lunar_points = token_data["lunarcrush"]["data"]

lunar_times = []
interactions = []
for point in lunar_points:
    if "interactions" in point:
        dt = datetime.datetime.fromtimestamp(point["time"], tz=datetime.timezone.utc)
        if start_time <= dt <= end_time:
            lunar_times.append(dt)
            interactions.append(point["interactions"])


# --- Plot the Data ---
fig, ax1 = plt.subplots(figsize=(12, 6))

# Plot CoinMarketCap price data on primary y-axis
ax1.plot(cmc_times, prices, 'b-', label='Price (USD)')
ax1.set_xlabel('Time')
ax1.set_ylabel('Price (USD)', color='b')
ax1.tick_params(axis='y', labelcolor='b')

# Plot LunarCrush interactions on secondary y-axis
ax2 = ax1.twinx()
ax2.plot(lunar_times, interactions, 'r-', label='Lunar Interactions')
ax2.set_ylabel('Interactions', color='r')
ax2.tick_params(axis='y', labelcolor='r')

fig.tight_layout()
plt.title(f"{token} Price and Lunar Interactions Over Time")
plt.show()

import json
import datetime
import matplotlib.pyplot as plt

def label_pump_and_dump(prices, times, pump_multiplier=2.0, dump_drop=0.8, dump_window=3):
    """
    Label the most significant pump and dump event in a price series.

    This function scans for candidate events defined by:
      - A pump: price rising from a local minimum to a local maximum
        with a multiplier >= pump_multiplier.
      - A dump: within 'dump_window' points after the peak,
        the price falls below dump_drop * (peak price).

    Returns:
      labels         : List of 0/1 values (1 indicates pump/dump period).
      best_candidate : Tuple (start_index, peak_index, pump_ratio) or None.
    """
    n = len(prices)
    candidates = []
    local_minima = []
    local_maxima = []

    # Identify local minima and maxima (simple method)
    for i in range(1, n - 1):
        if prices[i] < prices[i-1] and prices[i] <= prices[i+1]:
            local_minima.append(i)
        if prices[i] > prices[i-1] and prices[i] >= prices[i+1]:
            local_maxima.append(i)

    # For each local minimum, find the next local maximum
    for min_idx in local_minima:
        # Consider only maxima that occur after this minimum
        possible_max = [j for j in local_maxima if j > min_idx]
        if not possible_max:
            continue

        # Choose the highest peak
        max_idx = max(possible_max, key=lambda j: prices[j])
        pump_ratio = prices[max_idx] / prices[min_idx] if prices[min_idx] != 0 else 0

        if pump_ratio >= pump_multiplier:
            # Check for a rapid dump within `dump_window` after the peak
            end_range = min(n, max_idx + dump_window + 1)
            if any(prices[k] <= prices[max_idx] * dump_drop for k in range(max_idx+1, end_range)):
                candidates.append((min_idx, max_idx, pump_ratio))

    # Default: all zeros
    labels = [0] * n
    if not candidates:
        return labels, None

    # Pick the candidate with the highest pump ratio
    best_candidate = max(candidates, key=lambda x: x[2])
    start_idx, peak_idx, best_ratio = best_candidate

    # Label from the pump start to the peak (inclusive) as 1
    for i in range(start_idx, peak_idx + 1):
        labels[i] = 1

    return labels, best_candidate


# -------------------------------
#    Main Script
# -------------------------------
with open("../../combined_token_data.json", "r") as infile:
    combined_data = json.load(infile)

# Choose the token to analyze
token = "Tap"
token_data = combined_data.get(token)
if not token_data:
    raise ValueError(f"Token '{token}' not found in combined data.")

# --- Extract and process CoinMarketCap historical price data ---
cmc_quotes = token_data["coinmarketcap"]["historical_data"]["quotes"]

cmc_times = []
prices = []
for quote in cmc_quotes:
    ts_str = quote["timestamp"]
    dt = datetime.datetime.fromisoformat(ts_str.replace("Z", "+00:00"))
    cmc_times.append(dt)
    prices.append(quote["quote"]["USD"]["price"])

start_time = min(cmc_times)
end_time   = max(cmc_times)

# --- Extract and process LunarCrush interaction data ---
lunar_points = token_data["lunarcrush"]["data"]
lunar_times = []
interactions = []
for point in lunar_points:
    if "interactions" in point:
        dt = datetime.datetime.fromtimestamp(point["time"], tz=datetime.timezone.utc)
        if start_time <= dt <= end_time:
            lunar_times.append(dt)
            interactions.append(point["interactions"])

# -----------------------------
#  Label the pump & dump
# -----------------------------
labels, best_candidate = label_pump_and_dump(
    prices,
    cmc_times,
    pump_multiplier=2.0,  # require at least 2x increase
    dump_drop=0.8,        # price must fall to 80% of peak quickly
    dump_window=3         # within the next 3 data points
)

# -----------------------------
#  Inject the labels into JSON
# -----------------------------
for i, quote in enumerate(cmc_quotes):
    quote["pump_label"] = int(labels[i])  # 0 or 1

# (Optional) Save the modified JSON to a new file
with open("combined_token_data_with_labels.json", "w") as outfile:
    json.dump(combined_data, outfile, indent=4)
print("Saved updated JSON with pump labels to combined_token_data_with_labels.json.")

# -----------------------------
#  Plot the Data
# -----------------------------
fig, ax1 = plt.subplots(figsize=(12, 6))

# Plot CoinMarketCap price data on primary y-axis
ax1.plot(cmc_times, prices, 'b-', label='Price (USD)')
ax1.set_xlabel('Time')
ax1.set_ylabel('Price (USD)', color='b')
ax1.tick_params(axis='y', labelcolor='b')

# If a pump and dump event was detected, highlight it on the graph
if best_candidate is not None:
    start_idx, peak_idx, ratio = best_candidate
    ax1.axvspan(cmc_times[start_idx], cmc_times[peak_idx],
                color='yellow', alpha=0.3, label='Pump & Dump Region')

# Plot LunarCrush interactions on secondary y-axis
ax2 = ax1.twinx()
ax2.plot(lunar_times, interactions, 'r-', label='Lunar Interactions')
ax2.set_ylabel('Interactions', color='r')
ax2.tick_params(axis='y', labelcolor='r')

fig.tight_layout()
plt.title(f"{token} Price and Lunar Interactions Over Time")
plt.legend(loc='upper left')
plt.show()



import json
import datetime
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import classification_report, confusion_matrix

################################################################################
# 1. Choose Your Target Coin & Load Data
################################################################################

target_coin = "Tap"  # <-- Replace with whichever coin key you want to process

with open("../../combined_token_data.json", "r") as infile:
    combined_data = json.load(infile)

# Check if your target coin exists in the JSON
if target_coin not in combined_data:
    raise ValueError(f"Coin '{target_coin}' not found in combined_token_data.json")

token_data = combined_data[target_coin]  # all data for this one coin

################################################################################
# 2. Build a Single-Coin DataFrame
################################################################################

# --- 2.1 Extract CoinMarketCap daily price data ---
cmc_quotes = token_data["coinmarketcap"]["historical_data"]["quotes"]
price_records = []
for quote in cmc_quotes:
    ts_str = quote["timestamp"]
    # Convert "2021-01-01T00:00:00.000Z" style to Python datetime
    dt = datetime.datetime.fromisoformat(ts_str.replace("Z", "+00:00"))
    price = quote["quote"]["USD"]["price"]
    price_records.append({'time': dt, 'price': price})
df_price = pd.DataFrame(price_records).sort_values('time')

# --- 2.2 Extract LunarCrush data ---
lunar_points = token_data["lunarcrush"]["data"]
lunar_records = []
for point in lunar_points:
    if "interactions" in point:
        dt = datetime.datetime.fromtimestamp(point["time"], tz=datetime.timezone.utc)
        lunar_records.append({'time': dt, 'interactions': point["interactions"]})

if not lunar_records:
    # If no LunarCrush data, fill with 0
    df_single = df_price.copy()
    df_single['interactions'] = 0
else:
    df_lunar = pd.DataFrame(lunar_records).sort_values('time')
    # Merge the price and interactions data "as of" nearest timestamp
    df_single = pd.merge_asof(
        df_price, df_lunar,
        on='time', direction='nearest',
        tolerance=pd.Timedelta('1min')
    )
    # Forward-fill any missing interactions
    df_single['interactions'] = df_single['interactions'].fillna(method='ffill').fillna(0)

# --- 2.3 Basic Feature Engineering ---
df_single['price_pct_change'] = df_single['price'].pct_change().fillna(0)
df_single['interactions_pct_change'] = df_single['interactions'].pct_change().fillna(0)

# For demonstration, we add a naive pump_label. In your real pipeline,
# you'd apply your own label_pump_and_dump or advanced logic:
df_single['pump_label'] = 0
# Example: label 1 if daily price rose > 30%
df_single.loc[df_single['price_pct_change'] > 0.3, 'pump_label'] = 1

# Drop NAs and sort by time
df_single.dropna(inplace=True)
df_single.sort_values('time', inplace=True)

print(f"Data shape for {target_coin}: {df_single.shape}")
print("First few rows:")
print(df_single.head())

################################################################################
# 3. Create Sequences
################################################################################

feature_columns = ['price', 'interactions', 'price_pct_change', 'interactions_pct_change']
label_column = 'pump_label'
window_size = 30

def create_sequences(df, feature_cols, label_col, window_size=30):
    """
    Creates sliding windows of length 'window_size'.
    Each window (X) has shape (window_size, len(feature_cols)).
    The label (y) is the label of the last time step in that window.
    """
    data = df[feature_cols].values
    labels = df[label_col].values
    times = df['time'].values  # keep track of timestamps for plotting

    X, y, T = [], [], []
    for i in range(len(df) - window_size + 1):
        X_seq = data[i : i+window_size]
        y_seq = labels[i+window_size-1]  # label at the last step
        X.append(X_seq)
        y.append(y_seq)
        T.append(times[i+window_size-1])  # store the last timestamp
    return np.array(X), np.array(y), np.array(T)

X_seq, y_seq, time_seq = create_sequences(df_single, feature_columns, label_column, window_size)

print("X_seq shape:", X_seq.shape)  # (num_samples, window_size, num_features)
print("y_seq shape:", y_seq.shape)  # (num_samples,)

################################################################################
# 4. Train / Validation / Test Split
#    - We'll do a simple time-based split: first 60% train, next 20% val, last 20% test
################################################################################

total_samples = len(X_seq)
train_end = int(0.6 * total_samples)
val_end = int(0.8 * total_samples)

X_train = X_seq[:train_end]
y_train = y_seq[:train_end]
time_train = time_seq[:train_end]

X_val = X_seq[train_end:val_end]
y_val = y_seq[train_end:val_end]
time_val = time_seq[train_end:val_end]

X_test = X_seq[val_end:]
y_test = y_seq[val_end:]
time_test = time_seq[val_end:]

print("Train set:", X_train.shape, y_train.shape)
print("Val set:  ", X_val.shape, y_val.shape)
print("Test set: ", X_test.shape, y_test.shape)

# Check class distribution in training set:
unique, counts = np.unique(y_train, return_counts=True)
print("Training class distribution:", dict(zip(unique, counts)))

################################################################################
# 5. Build an LSTM Classifier
################################################################################

model = Sequential()
model.add(LSTM(64, input_shape=(window_size, len(feature_columns)), activation='tanh'))
model.add(Dense(1, activation='sigmoid'))  # binary classification output

model.compile(
    optimizer=Adam(learning_rate=1e-3),
    loss='binary_crossentropy',
    metrics=['accuracy']
)

model.summary()

################################################################################
# 6. Handle Class Imbalance with Class Weights
################################################################################

num_0 = sum(y_train == 0)
num_1 = sum(y_train == 1)

if num_1 == 0:
    # Edge case: if there's no pump in the training set, no supervised approach will truly learn to detect pumps
    class_weight = {0: 1.0, 1: 1.0}
else:
    class_weight = {
        0: 1.0,
        1: (num_0 / float(num_1))
    }

print("Class weights:", class_weight)

################################################################################
# 7. Train the Model
################################################################################

history = model.fit(
    X_train, y_train,
    epochs=5,
    batch_size=64,
    validation_data=(X_val, y_val),
    class_weight=class_weight,
    verbose=1
)

################################################################################
# 8. Evaluate the Model on the Test Set
################################################################################

y_test_pred_prob = model.predict(X_test).flatten()
threshold = 0.5
y_test_pred = (y_test_pred_prob >= threshold).astype(int)

print("\nConfusion Matrix on Test Set:")
print(confusion_matrix(y_test, y_test_pred))
print("\nClassification Report on Test Set:")
print(classification_report(y_test, y_test_pred, digits=4))

################################################################################
# 9. Visualize Predictions vs. Price
################################################################################

# We have 'time_test' which is the final timestamp of each sequence
# Let's build a small DataFrame for the test predictions
df_result_test = pd.DataFrame({
    'time': time_test,
    'true_label': y_test,
    'pred_label': y_test_pred
})

# Merge back to get the price for plotting
# We'll do a left-join on "time"
df_test_plot = pd.DataFrame({'time': pd.to_datetime(df_result_test['time'])})

# If your df_single['time'] is tz-naive, localize if needed:
df_single['time'] = pd.to_datetime(df_single['time']).dt.tz_localize(None)
df_test_plot['time'] = pd.to_datetime(df_test_plot['time']).dt.tz_localize(None)

df_test_plot = pd.merge(
    df_test_plot,
    df_single[['time','price']],
    on='time',
    how='left'
)

# Join in the predicted labels
df_test_plot = df_test_plot.join(
    df_result_test.set_index('time')['pred_label'],
    on='time'
)

# Plot
plt.figure(figsize=(12, 6))
plt.plot(df_test_plot['time'], df_test_plot['price'], label=f'{target_coin} Price (Test Window)')

pump_df = df_test_plot[df_test_plot['pred_label'] == 1]
plt.scatter(
    pump_df['time'],
    pump_df['price'],
    color='red',
    marker='o',
    label='Predicted Pump',
    s=100
)

plt.title(f"{target_coin} Price Over Time (Test Set) with Predicted Pumps")
plt.xlabel('Time')
plt.ylabel('Price (USD)')
plt.legend()
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Print some stats about predictions
print("\nPrediction Stats:")
print(f"Total predictions in test set: {len(df_result_test)}")
print(f"Predicted pumps (1): {sum(df_result_test['pred_label'] == 1)}")
pct_pump = sum(df_result_test['pred_label'] == 1) / len(df_result_test) * 100
print(f"Percentage predicted pumps: {pct_pump:.2f}%")

print("\nFirst few rows of df_test_plot for debugging:")
print(df_test_plot.head())







